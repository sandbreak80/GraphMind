# EminiPlayer QA Automation Rules
#LAST CONVERSATION - cursor-agent --resume=422da8fd-5c19-43fd-8cdc-41ad2736a26c

## Test Requirements
- All new features must include unit tests
- All API changes must include integration tests
- All user workflows must include e2e tests
- Test coverage must be >80%
- All tests must pass before merge

## Test Execution
- Run tests automatically on every change
- Run full test suite on pull requests
- Run performance tests on main branch
- Run security scans on all changes

## Test Quality
- Tests must be deterministic and reliable
- Tests must clean up after themselves
- Tests must not depend on external services
- Tests must be fast and efficient

## Test Documentation
- All test files must have docstrings
- Complex test logic must be commented
- Test data must be clearly documented
- Test failures must be actionable

## Automated Actions
- On code change: Run unit tests
- On pull request: Run full test suite
- On merge to main: Run performance tests
- On deployment: Run security scans
- On test failure: Block merge/deployment

## QA Suite Commands
- Run comprehensive tests: `python3 tests/integration/test_comprehensive_suite_v2.py`
- Run performance tests: `python3 tests/performance/test_performance_suite.py`
- Run quick validation: `python3 tests/integration/test_quick_validation.py`
- Run full QA suite: `./scripts/run_qa_suite.sh`

## Test Categories
1. **Comprehensive Tests**: All functionality including new features
2. **Performance Tests**: Response times, load handling, memory usage
3. **Quick Validation**: Data source exclusivity and basic functionality
4. **Security Tests**: Authentication, authorization, input validation

## Test Data Requirements
- Use realistic test data
- Test with different models (llama3.2:3b, llama3.1:latest)
- Test all chat modes (RAG, Web, Obsidian, Research)
- Test system prompt management
- Test model switching functionality
- Test chat export functionality
- Test response time measurement

## Performance Benchmarks
- Single request response time: < 30 seconds
- Concurrent requests: < 45 seconds max
- API endpoints: < 5 seconds
- Memory stability: < 50% degradation
- Success rate: > 80%

## Test Environment
- Base URL: http://localhost:3001
- Authentication: admin/admin123
- Timeout: 30 seconds for chat requests
- Timeout: 10 seconds for API requests

## Code Quality
- Follow PEP 8 for Python code
- Use TypeScript for frontend code
- Add comprehensive docstrings
- Include type hints for Python
- Use meaningful variable names

## Documentation
- Update documentation with every change
- Include examples in documentation
- Keep architecture docs current
- Document all API changes
- Maintain user guides

## Security
- Validate all user inputs
- Use secure authentication
- Protect sensitive data
- Regular security scans
- Follow security best practices

## Performance
- Monitor response times
- Optimize database queries
- Use efficient algorithms
- Cache when appropriate
- Profile performance regularly

## Memory Management
- Clean up resources properly
- Monitor memory usage
- Use efficient data structures
- Avoid memory leaks
- Profile memory usage

## Error Handling
- Handle all exceptions gracefully
- Provide meaningful error messages
- Log errors appropriately
- Include error recovery
- Test error scenarios

## Data Management
- Validate data integrity
- Use transactions when needed
- Backup data regularly
- Clean up old data
- Monitor data growth